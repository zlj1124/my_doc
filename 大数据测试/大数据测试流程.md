<!--
 * @Descripttion: 
 * @Author: zlj
 * @Date: 2021-11-24 13:49:47
-->
# 大数据测试实现被分成三个步骤：

## 1.大数据阶段验证
♦ 来自各方面的数据资源应该被验证，来确保正确的数据被加载进系统
♦ 将源数据与推送到Hadoop系统中的数据进行比较，以确保它们匹配
♦ 验证正确的数据被提取并被加载到HDFS正确的位置

该阶段可以使用工具Talend或Datameer，进行数据阶段验证

## 2. "MapReduce"验证

在这个阶段，测试者在每个节点上进行业务逻辑验证，然后在运行多个节点后验证它们，确保如下操作的正确性：

♦ Map与Reduce进程正常工作
♦ 在数据上实施数据聚合或隔离规则
♦ 生成键值对
♦ 在执行Map和Reduce进程后验证数据

## 3.输出阶段验证

生成输出数据文件，同时把文件移到一个EDW(企业数据仓库)中或着把文件移动到任何其他基于需求的系统中。在第三阶段的活动包括：

♦ 检查转换规则被正确应用
♦ 检查数据完整性和成功的数据加载到目标系统中
♦ 通过将目标数据与HDFS文件系统数据进行比较来检查没有数据损坏

# 数据收集

日志采集：
收集企业业务平台日常产生的大量日志数据，以供后续离线和在线的大数据分析系统使用。高可用性、高可靠性和可扩展性是日志收集系统所具有的基本特征。收集工具：「Flume 、Logstash、Kibana」

网络采集：指通过网络爬虫或网站公开API等方式从网站获取数据信息的过程

# 数据存储
分布式文件系统：HDFS
数据分为结构化数据、半结构化数据、非结构化数据
结构化数据就关系型数据库表示和存储的，像我们存储到 mysql、Oracle 表里面的数据就是结构化数据，「MySQL、Oracle」
半结构化数据就是结构化数据的一种形式，像我们平时用到的 XML、JSON 属于常见半结构化数据，「GFS、HDFS」
非结构化数据就是各种文档、图片、视频/音频 「Hbase 、MongoDB」

# 数据处理
批处理：对离线数据进行处理，「对数据的时效性要求不高」所以对应的就是批处理，处理框架有 「Hadoop MapReduce、Spark、Flink」 等

流处理：对实时数据进行处理，「对数据的时效性非常高」所以对应的就是流处理，即在接收数据的同时就对其进行处理，处理框架有 「Storm、Spark Streaming、Flink Streaming」 等
